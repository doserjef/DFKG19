rm(list = ls())
source("../../code/04-psdIntYearDesignMatrix.R")
setwd("~/Dropbox/DFG19/models/psd-intervention-year/")
library(coda)

# Main Program ------------------------------------------------------------

# Multivariate normal random number generator
rmvn <- function(n, mu=0, V = matrix(1)){
  p <- length(mu)
  if(any(is.na(match(dim(V),p))))
    stop("Dimension problem!")
  D <- chol(V)
  t(matrix(rnorm(n*p), ncol=p)%*%D + rep(mu,rep(n,p)))
}

# Inverse Gamma random number generator
rigamma <- function(n, a, b){
  1/rgamma(n = n, shape = a, rate = b)
} 

# Logit data transformation
logit <- function(theta, a, b) {
  log((theta-a)/(b-theta))
}

# Inverse logit transformation
logit.inv <- function(z, a, b) {
  b-(b-a)/(1+exp(z))
}


# Params ------------------------------------------------------------------

n.samples <- 100
n.sites <- length(unique(ordered.long.dat$recording.site))
psd <- ordered.long.dat$value
y <- as.matrix(logit(psd, a = 0, b = 1))
ind <- ordered.long.dat$ind
n.ind <- length(unique(ind))
n <- nrow(X)
n.alpha <- dim(X)[2]
n.beta <- 1
s <- rep(0, n.ind)
for (i in 1:n.ind) {
  curr <- sum(ind == i)
  s[i] <- curr / n.psd
}
ind.beta <- rep(1:n.ind, each = n.psd)

# Priors ------------------------------------------------------------------

# Normal prior on fixed regression coefficients
alpha.mu <- rep(0, n.alpha)
alpha.var <- rep(10000, n.alpha)

# Normal prior on random individual effects
beta.mu <- matrix(0, nrow = n.ind * n.psd, ncol = n.beta)

# Inverse gamma prior on process variance
sigma.sq.a <- 2
sigma.sq.b <- 1

# Inverse wishart prior on 10 x 10 covariance matrix
R <- diag(0.1, n.psd)
r <- 3


# Starting values ---------------------------------------------------------

alpha <- rep(0, n.alpha)
beta <- matrix(0, nrow = n.ind * n.psd, ncol = n.beta)
sigma.sq <- 1
lambda <- diag(.1, n.psd)
y.hat <- rep(0, n)


# Sampler Prep ------------------------------------------------------------

alpha.samples <- matrix(0, nrow = n.alpha, ncol = n.samples)
beta.samples <- matrix(0, nrow = n.ind * n.psd, ncol = n.samples)
fitted.samples <- matrix(0, nrow = n, ncol = n.samples)
lambda.samples <- matrix(0, nrow = n.psd * n.psd, ncol = n.samples)
sigma.sq.samples <- rep(0, n.samples)
XTX <- t(X)%*%X

# Gibbs Sampler -----------------------------------------------------------

for (k in 1:n.samples) {
  
  # Update alpha samples
  V <- chol2inv(chol(XTX / sigma.sq + diag(1/alpha.var, n.alpha)))
  sum.diffs <- rep(0, n.alpha)
  for (i in 1:n.ind) {
    X.i <- X[ind == i, ]
    y.i <- y[ind == i]
    # Faster way to code the indicator matrix described in the sampler
    beta.i <- rep(beta[ind.beta ==i], times = s[i]) 
    sum.diffs <- sum.diffs + t(X.i) %*% (y.i - beta.i)
  }
  v <- sum.diffs / sigma.sq
  alpha <- rmvn(1, V%*%v, V)
  
  # Update beta samples
  for (i in 1:n.ind) {
    X.i <- X[ind == i, ]
    y.i <- y[ind == i]
    V <- chol2inv(chol(diag(s[i] / sigma.sq, n.psd) + chol2inv(chol(lambda))))
    # Indicator matrix (aka w)
    ones.mat <- diag(1, n.psd)
    ones.mat <- matrix(rep(ones.mat, s[i]), nrow = n.psd * s[i], 
                       ncol = n.psd, byrow = T)
    v <-  t(ones.mat) %*% (y.i - X.i %*% alpha) / sigma.sq
    beta[((i-1)*n.psd + 1):(i*n.psd), ] <- rmvn(1, V%*%v, V)
  }
  
  # Update sigma.sq samples
  a <- sigma.sq.a + 5 * sum(s)
  b.sum <- 0
  for (i in 1:n.ind) {
    X.i <- X[ind == i, ]
    y.i <- y[ind == i]
    # This is a faster way to code the identicator matrix described in the sampler
    beta.i <- rep(beta[ind.beta ==i], times = s[i]) 
    b.sum <- b.sum + t((y.i - (X.i%*%alpha + beta.i))) %*% 
      (y.i - (X.i%*%alpha + beta.i))
  }
  b <- sigma.sq.b + .5 * b.sum
  sigma.sq <- rigamma(1, a, b)
  
  # Update lambda (inverse) samples
  a <- n.ind + r
  beta.sum <- matrix(0, ncol = n.psd, nrow = n.psd)
  for (i in 1:n.ind) {
    beta.sum <- beta.sum + (beta[((i-1)*n.psd + 1):(i*n.psd), ] %*% 
                              t(beta[((i-1)*n.psd + 1):(i*n.psd), ]))
  }
  b <- chol2inv(chol(beta.sum + r*R))
  # Note that the rWishart function returns a 3D array, so you need to 
  # extract only the first two dimensions
  lambda <- chol2inv(chol(rWishart(1, a, b)[, , 1]))
  
  # Fitted values
  curr.length <- 0 
  for (i in 1:n.ind) {
    x.i <- X[ind == i, ]
    V <- diag(sigma.sq, s[i] * n.psd)
    beta.i <- rep(beta[ind.beta ==i], times = s[i])
    mu <- x.i %*% alpha + beta.i
    y.hat[(curr.length + 1):(curr.length + s[i]*n.psd)] <- 
      rmvn(n = 1, mu = mu, V = V)
    curr.length <- curr.length + s[i]*n.psd
  }
  
  # Save Samples
  alpha.samples[, k] <- alpha
  beta.samples[, k] <- beta[, 1]
  lambda.samples[, k] <- c(lambda)
  sigma.sq.samples[k] <- sigma.sq
  fitted.samples[, k] <- y.hat
  
  print(paste(k/n.samples * 100, " percent complete", sep = ""))
  
}


# Summary -----------------------------------------------------------------

# Look at process varaince
plot(mcmc(sigma.sq.samples), density = FALSE)

# Lambda covariance matrix
summary(mcmc(t(lambda.samples)))

# Burn in
burn.in <- floor(0.5 * n.samples)
sub <- (burn.in+1):n.samples

y.hat.mean <- apply(fitted.samples[, sub], 1, mean)
plot(y[, 1], y.hat.mean)
lines(y.hat.mean, y.hat.mean, col = 'red')

resids <- y.hat.mean - y[, 1]
plot(resids, type = 'l')

summary(mcmc(window(t(alpha.samples), start = burn.in)))

        